# -*- coding: utf-8 -*-
"""Scrapppingdocs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D2xgGkoIH20_YOIIXO04vEiILmNbj-mY
"""

from bs4 import BeautifulSoup
import requests

url="https://www.shl.com/products/product-catalog/"
requests.get(url)

from types import NoneType
import pandas as pd

page=requests.get(url)
soup=BeautifulSoup(page.text,'html')

#print(soup.prettify())

l1=soup.find_all('tr')

l1[1]

links=[]
for a in soup.select('a.pagination__link'):
    links.append(a['href'])

links

def explore_urls(link_list):
  links=link_list

  all_rows=[]
  for url in links:

    page=requests.get(f'https://www.shl.com{url}')
    soup=BeautifulSoup(page.text,'html')
    l1=soup.find_all('tr')
    for a in soup.select('a.pagination__link'):
      if a['href'] not in links:

        links.append(a['href'])
      else:
        continue
    print(links)
    for row in l1:
      rw_d=row.find_all('td')
      #print(rw_d)
      if rw_d==[]:
        continue
      link=rw_d[0].find('a')['href']
      span1 = rw_d[1].find('span')
      span2 = rw_d[2].find('span')

      z1 = 'T' if span1 and span1.has_attr('class') else 'F'
      z2 = 'T' if span2 and span2.has_attr('class') else 'F'

      #print(rw_d[3].find('span'))
      id_w=[data.text.strip() for data in rw_d]
      combined=''.join(id_w[-1])
      all_rows.append([link,z1,z2,combined])
  return all_rows

explore_urls(links)

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

def explore_urls(link_list):
    base_url = "https://www.shl.com"
    visited_urls = set()
    to_visit = link_list.copy()  # Create a copy to avoid modifying during iteration
    all_rows = []

    while to_visit:
        url = to_visit.pop(0)
        if url in visited_urls:
            continue

        visited_urls.add(url)

        try:
            # Add a small delay to be respectful to the server
            time.sleep(1)

            full_url = urljoin(base_url, url)
            print(f"Scraping: {full_url}")

            page = requests.get(full_url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })

            # Check if request was successful
            if page.status_code != 200:
                print(f"Failed to fetch {full_url}, status code: {page.status_code}")
                continue

            soup = BeautifulSoup(page.text, 'html.parser')

            # Find pagination links
            for a in soup.select('a.pagination__link'):
                if a.has_attr('href'):
                    new_url = a['href']
                    if new_url not in visited_urls and new_url not in to_visit:
                        to_visit.append(new_url)

            # Process table rows
            l1 = soup.find_all('tr')
            for row in l1:
                rw_d = row.find_all('td')

                # Skip empty rows
                if not rw_d:
                    continue

                try:
                    # Extract link
                    link_element = rw_d[0].find('a')
                    link = link_element['href'] if link_element and link_element.has_attr('href') else None

                    # Extract spans
                    span1 = rw_d[1].find('span') if len(rw_d) > 1 else None
                    span2 = rw_d[2].find('span') if len(rw_d) > 2 else None

                    z1 = 'T' if span1 and span1.has_attr('class') else 'F'
                    z2 = 'T' if span2 and span2.has_attr('class') else 'F'

                    # Extract text
                    id_w = [data.text.strip() for data in rw_d]
                    combined = ''.join(id_w[-1]) if id_w else ''

                    all_rows.append([link, z1, z2, combined])

                except IndexError as e:
                    print(f"Error processing row: {e}")
                    continue

        except Exception as e:
            print(f"Error processing {url}: {e}")
            continue

    return all_rows

# Example usage
initial_links = ['/products/product-catalog/']
results = explore_urls(initial_links)

df=pd.DataFrame(results,columns=['Pre-packaged Job Solutions','Remote Testing','Adaptive/IRT','Test Type'])

df

def extract_p_content(relative_url):
    base_url = "https://www.shl.com"
    full_url = urljoin(base_url, relative_url)

    try:
        response = requests.get(full_url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract paragraphs for each heading
        description_ps = [p.text.strip() for p in soup.select('h4:contains("Description") ~ p')]
        job_levels_ps = [p.text.strip() for p in soup.select('h4:contains("Job levels") ~ p')]
        languages_ps = [p.text.strip() for p in soup.select('h4:contains("Languages") ~ p')]
        assessment_length_ps = [p.text.strip() for p in soup.select('h4:contains("Assessment length") ~ p')]

        # Join paragraphs for each category
        description = ' '.join(description_ps)
        job_levels = ' '.join(job_levels_ps)
        languages = ' '.join(languages_ps)
        assessment_length = ' '.join(assessment_length_ps)

        # Return as dictionary for easy column assignment
        return {
            'Description': description,
            'Job_Levels': job_levels,
            'Languages': languages,
            'Assessment_Length': assessment_length
        }
    except Exception as e:
        return {
            'Description': f"Error: {str(e)}",
            'Job_Levels': "",
            'Languages': "",
            'Assessment_Length': ""
        }

# # Apply to DataFrame
df[['Description', 'Job_Levels', 'Languages', 'Assessment_Length']] = df['Pre-packaged Job Solutions'].apply(
     lambda x: pd.Series(extract_p_content(x))
 )

extract_p_content(df.iloc[0][0])

df

df[df.columns[-1]]

df.to_csv('./impscrapping.csv')

import pandas as pd

t1=pd.read_csv('/content/assesments.csv')

t1=t1.head(3)

t1.to_csv('sample.csv')

import pandas as pd

t1=pd.read_csv('/content/assesments.csv')

t1.columns

